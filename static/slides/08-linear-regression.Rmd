---
title: "Linear Regression in R"
author: "Dr. D'Agostino McGowan"
output:
  xaringan::moon_reader:
    css: "slides.css"
    logo: img/icon.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---



```{r child = "setup.Rmd"}
```

layout: true

<div class="my-footer">
<span>
Dr. Lucy D'Agostino McGowan
</span>
</div> 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
library(tidyverse)
library(tidymodels)
library(Stat2Data)
library(countdown)
data(Sparrows)
```
---

## Let's look at an example

Let's look at a sample of 116 sparrows from Kent Island. We are interested in the relationship between `Weight` and `Wing Length`

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 2}
ggplot(Sparrows, aes(x = WingLength, y = Weight)) + 
  geom_point() + 
  labs(title = "The relationship between weight and wing length", 
       subtitle = "Savannah Sparrows on Kent Island",
       x = "Wing length") + 
  geom_smooth(method = "lm", fill = "blue")
```

* the **standard error** of $\hat{\beta_1}$ ( $SE_{\hat{\beta}_1}$ ) is how much we expect the sample slope to vary from one random sample to another.

---

## Sparrows

.question[
How can we quantify how much we'd expect the slope to differ from one random sample to another?
]

.small[
```{r, highlight.output = 5}
linear_reg() %>%
  set_engine("lm") %>%
  fit(Weight ~ WingLength, data = Sparrows) %>%
  tidy() #<<
```
]

---

## Sparrows

.question[
How do we interpret this?
]

.small[
```{r, highlight.output = 5}
linear_reg() %>%
  set_engine("lm") %>%
  fit(Weight ~ WingLength, data = Sparrows) %>%
  tidy()
```
]

---

## Sparrows

.question[
How do we interpret this?
]

.small[
```{r, highlight.output = 5}
linear_reg() %>%
  set_engine("lm") %>%
  fit(Weight ~ WingLength, data = Sparrows) %>%
  tidy()
```
]

* "the sample slope is more than 13 standard errors above a slope of zero"

---

## Sparrows

.question[
How do we know what values of this statistic are worth paying attention to?
]

.small[
```{r, highlight.output = 5}
linear_reg() %>%
  set_engine("lm") %>%
  fit(Weight ~ WingLength, data = Sparrows) %>%
  tidy()
```
]

---

## Sparrows

.question[
How do we know what values of this statistic are worth paying attention to?
]

.small[
```{r, highlight.output = 5}
linear_reg() %>%
  set_engine("lm") %>%
  fit(Weight ~ WingLength, data = Sparrows) %>%
  tidy()
```
]

* confidence intervals
* p-values

---

## Sparrows

.question[
How do we know what values of this statistic are worth paying attention to?
]

.small[
```{r, highlight.output = 5}
linear_reg() %>%
  set_engine("lm") %>%
  fit(Weight ~ WingLength, data = Sparrows) %>%
  tidy(conf.int = TRUE) #<<
```
]

* confidence intervals
* p-values

---
class: inverse

## `r fontawesome::fa("laptop")` `Application Exercise`

1. Fit a linear model using the `mtcars` data frame predicting miles per gallon (`mpg`) from (`wt`)
2. Pull out the coefficients and confidence intervals using the `tidy()` function demonstrated. How do you interpret these?

`r countdown(4)`
---

## Sparrows

.question[
How are these statistics distributed under the null hypothesis?
]

.small[
```{r, highlight.output = 5}
linear_reg() %>%
  set_engine("lm") %>%
  fit(Weight ~ WingLength, data = Sparrows) %>%
  tidy() 
```
]

---

```{r, echo = FALSE}
gen_null_stat <- function() { #<<
  null_sparrow_data <- data.frame(
    WingLength = rnorm(10, 27, 4),
    Weight = rnorm(10, 14, 3)
  )
  lm(Weight ~ WingLength, data = null_sparrow_data) %>%
    tidy() %>%
    filter(term == "WingLength") %>%
    select("statistic")
} #<<
```

```{r, echo = FALSE, cache = TRUE}
null_stats <- map_df(1:10000, ~ gen_null_stat())
normal_data <- null_stats %>%
  mutate(y_t = dt(statistic, df = 18),
         y_norm = dnorm(statistic, 0, 1))
```


## Sparrows

```{r, echo = FALSE}
null_stats <- map_df(1:10000, ~ gen_null_stat())
```

```{r, echo = FALSE}
ggplot(null_stats, aes(x = statistic)) +
  geom_histogram(bins = 70) + 
  labs(title = "Histogram of statistics under the null")
```

* I've generated some data under a null hypothesis where $n = 20$

---

## Sparrows


```{r, echo = FALSE}
ggplot(null_stats) +
  geom_histogram(aes(x = statistic, y = ..density..), bins = 70) + 
  geom_line(data = normal_data, aes(x = statistic, y = y_t), color = "green") + 
  labs(title = "Histogram of statistics under the null",
       subtitle = "Overlaid with a t-distribution")
```

* this is a **t-distribution** with **n-p-1** degrees of freedom.

---

## Sparrows

The distribution of test statistics we would expect given the **null hypothesis is true**, $\beta_1 = 0$, is **t-distribution** with **n-2** degrees of freedom.

```{r, echo = FALSE}
ggplot(null_stats) +
  geom_histogram(aes(x = statistic, y = ..density..), bins = 70) + 
  geom_line(data = normal_data, aes(x = statistic, y = y_t), color = "green") + 
    labs(title = "Histogram of statistics under the null",
       subtitle = "Overlaid with a t-distribution")
```

---

## Sparrows

```{r, echo = FALSE, fig.height = 2}
null_stats_bigger <- data.frame(
  statistic = rt(10000, df = 114)
)

```

```{r, echo = FALSE}
ggplot(null_stats_bigger) +
  geom_histogram(aes(x = statistic, y = ..density..), bins = 70) + 
  geom_vline(xintercept = 13.463, lwd = 1.5) + 
    labs(title = "Histogram of statistics under the null",
         subtitle = "t-distribution with 114 degrees of freedom")
```

---

## Sparrows

.question[
How can we compare this line to the distribution under the null?
]

```{r, echo = FALSE, fig.height = 2}
ggplot(null_stats_bigger) +
  geom_histogram(aes(x = statistic, y = ..density..), bins = 70) + 
  geom_vline(xintercept = 13.463, lwd = 1.5) + 
    labs(title = "Histogram of statistics under the null",
         subtitle = "t-distribution with 114 degrees of freedom")
```

--

* p-value

---

class: middle

# p-value

The probability of getting a statistic as extreme or more extreme than the observed test statistic **given the null hypothesis is true**

---

## Sparrows

```{r, echo = FALSE, fig.height = 2}
ggplot(null_stats_bigger) +
  geom_histogram(aes(x = statistic, y = ..density..), bins = 70) + 
  geom_vline(xintercept = 13.463, lwd = 1.5) + 
  geom_vline(xintercept = -13.463, lwd = 1.5) +
    labs(title = "Histogram of statistics under the null",
         subtitle = "t-distribution with 114 degrees of freedom")
```

.small[
```{r, highlight.output = 5}
linear_reg() %>%
  set_engine("lm") %>%
  fit(Weight ~ WingLength, data = Sparrows) %>%
  tidy()
```
]

---

## Return to generated data, n = 20

```{r, echo = FALSE}
null_stats$shade <- ifelse(null_stats$statistic > 1.5 | null_stats$statistic < -1.5, TRUE, FALSE)
```

```{r, echo = FALSE, fig.height = 2}
ggplot(null_stats) +
  geom_histogram(aes(x = statistic, fill = shade), bins = 70) + 
  geom_vline(xintercept = 1.5, lwd = 1.5) + 
  geom_vline(xintercept = -1.5, lwd = 1.5) +
    labs(title = "Histogram of statistics under the null") + 
  theme(legend.position = "none")
```

* Let's say we get a statistic of **1.5** in a sample
---

## Let's do it in R!

The proportion of area less than 1.5

```{r, echo = FALSE, fig.height = 2}
null_stats$shade <- ifelse(null_stats$statistic < 1.5, TRUE, FALSE)
ggplot(null_stats) +
  geom_histogram(aes(x = statistic, fill = shade), bins = 70) + 
  geom_vline(xintercept = 1.5, lwd = 1.5) + 
    labs(title = "Histogram of statistics under the null") + 
  theme(legend.position = "none")
```


```{r}
pt(1.5, df = 18)
```

---

## Let's do it in R!

The proportion of area **greater** than 1.5

```{r, echo = FALSE, fig.height = 2}
null_stats$shade <- ifelse(null_stats$statistic > 1.5, TRUE, FALSE)
ggplot(null_stats) +
  geom_histogram(aes(x = statistic, fill = shade), bins = 70) + 
  geom_vline(xintercept = 1.5, lwd = 1.5) + 
    labs(title = "Histogram of statistics under the null") + 
  theme(legend.position = "none")
```


```{r}
pt(1.5, df = 18, lower.tail = FALSE)
```
---

## Let's do it in R!

The proportion of area **greater** than 1.5 **or** **less** than -1.5.

```{r, echo = FALSE, fig.height = 2}
null_stats$shade <- ifelse(null_stats$statistic > 1.5 | null_stats$statistic < - 1.5, TRUE, FALSE)
ggplot(null_stats) +
  geom_histogram(aes(x = statistic, fill = shade), bins = 70) + 
  geom_vline(xintercept = 1.5, lwd = 1.5) + 
    geom_vline(xintercept = -1.5, lwd = 1.5) + 
    labs(title = "Histogram of statistics under the null") + 
  theme(legend.position = "none")
```

--

```{r}
pt(1.5, df = 18, lower.tail = FALSE) * 2
```

---

class: middle

# p-value

The probability of getting a statistic as extreme or more extreme than the observed test statistic **given the null hypothesis is true**

---

## Hypothesis test

* **null hypothesis** $H_0: \beta_1 = 0$ 
* **alternative hypothesis** $H_A: \beta_1 \ne 0$
--

* **p-value**: 0.15
--

* Often, we have an $\alpha$-level cutoff to compare this to, for example **0.05**. Since this is greater than **0.05**, we **fail to reject the null hypothesis**

---

class: inverse

## `r fontawesome::fa("laptop")` `Application Exercise`

 Using the linear model you fit previously (`mpg` from `wt` using the `mtcars` data) - calculate the p-value for the coefficient for weight? Interpret this value. What is the null hypothesis? What is the alternative hypothesis? Do you reject the null?

`r countdown(4)`

---

class: middle

# confidence intervals

If we use the same sampling method to select different samples and computed an interval estimate for each sample, we would expect the true population parameter ( $\beta_1$ ) to fall within the interval estimates 95% of the time.

---

# Confidence interval

.center[
$\Huge \hat\beta_1 \pm t^∗ \times SE_{\hat\beta_1}$
]

---

# Confidence interval

.center[
$\Huge \hat\beta_1 \pm t^∗ \times SE_{\hat\beta_1}$
]

* $t^*$ is the critical value for the $t_{n−p-1}$ density curve to obtain the desired confidence level
--

* Often we want a **95% confidence level**.  

---

## Let's do it in R!

.small[
```{r, highlight.output = 5}
linear_reg() %>%
  set_engine("lm") %>%
  fit(Weight ~ WingLength, data = Sparrows) %>%
  tidy(conf.int = TRUE)
```
]

* $t^* = t_{n-p-1} = t_{114} = 1.98$
--

* $LB = 0.47 - 1.98\times 0.0347 = 0.399$
* $UB = 0.47+1.98 \times 0.0347 = 0.536$

---

class: middle

# confidence intervals

If we use the same sampling method to select different samples and computed an interval estimate for each sample, we would expect the true population parameter ( $\beta_1$ ) to fall within the interval estimates 95% of the time.

---

## Linear Regression Questions

* `r emo::ji("heavy_check_mark")` Is there a relationship between a response variable and predictors? 
* `r emo::ji("heavy_check_mark")` How strong is the relationship? 
* `r emo::ji("heavy_check_mark")` What is the uncertainty? 
* How accurately can we predict a future outcome?

---

## Sparrows

.question[
Using the information here, how could I predict a new sparrow's weight if I knew the wing length was 30?
]

```{r}
linear_reg() %>%
  set_engine("lm") %>%
  fit(Weight ~ WingLength, data = Sparrows) %>%
  tidy()
```

--

* $1.37 + 0.467 \times 30 = 15.38$

---

## Linear Regression Accuracy

.question[
What is the residual sum of squares again?
]

* Note: In previous classes, this may have been referred to as SSE (sum of squares error), the book uses RSS, so we will stick with that!

--

$$RSS = \sum(y_i - \hat{y}_i)^2$$

--

* The **total sum of squares** represents the variability of the outcome, it is equivalent to the variability described by the **model** plus the remaining **residual sum of squares**

$$TSS = \sum(y_i - \bar{y})^2$$
---

## Linear Regression Accuracy

* There are many ways "model fit" can be assessed. Two common ones are:
  * Residual Standard Error (RSE)
  * $R^2$ - the fraction of the variance explained
--

* $RSE = \sqrt{\frac{1}{n-p-1}RSS}$
--

* $R^2 = 1 - \frac{RSS}{TSS}$

---

## Linear Regression Accuracy

.question[
What could we use to determine whether at least one predictor is useful?
]

--

$$F = \frac{(TSS - RSS)/p}{RSS/(n-p-1)}\sim F_{p,n-p-1}$$
We can use a F-statistic!

---

## Let's do it in R!

.small[
```{r}
lm_fit <- linear_reg() %>% #<<
  set_engine("lm") %>%
  fit(Weight ~ WingLength, data = Sparrows)

glance(lm_fit$fit) #<<
```
]

---

class: inverse

## `r fontawesome::fa("laptop")` `Application Exercise`

Using the model previously fit (using the `mtcars` data predicting miles per gallon from weight), pull out the F-statistic and $R^2$ using the `glance()` function. Interpret these values.

`r countdown(4)`

---

## Additional Linear Regression Topics

* Polynomial terms
* Interactions
* Outliers
* Non-constant variance of error terms
* High leverage points
* Collinearity

_Refer to Chapter 3 for more details on these topics if you need a refresher._
