<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Cross validation</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr.Â Dâ€™Agostino McGowan" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link rel="stylesheet" href="slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Cross validation
### Dr.Â Dâ€™Agostino McGowan

---






layout: true

&lt;div class="my-footer"&gt;
&lt;span&gt;
Dr. Lucy D'Agostino McGowan &lt;i&gt;adapted from slides by Hastie &amp; Tibshirani&lt;/i&gt;
&lt;/span&gt;
&lt;/div&gt; 



---

## Cross validation

### ðŸ’¡ Big idea

* We have determined that it is sensible to use a _test_ set to calculate metrics like prediction error

--

.question[
Why?
]

---

## Cross validation 

### ðŸ’¡ Big idea

* We have determined that it is sensible to use a _test_ set to calculate metrics like prediction error

.question[
How have we done this so far?
]

---

## Cross validation

### ðŸ’¡ Big idea

* We have determined that it is sensible to use a _test_ set to calculate metrics like prediction error
* What if we don't have a seperate data set to test our model on?
--

* ðŸŽ‰ We can use **resampling** methods to **estimate** the test-set prediction error

---

## Training error versus test error

.question[
What is the difference? Which is typically larger?
]

--

* The **training error** is calculated by using the same observations used to fit the statistical learning model
--

* The **test error** is calculated by using a statistical learning method to predict the response of **new** observations
--

* The **training error rate** typically _underestimates_ the true prediction error rate

---

![](img/05/model-complexity.png)

---

## Estimating prediction error

* Best case scenario: We have a large data set to test our model on 
--

* This is not always the case!

--

ðŸ’¡ Let's instead find a way to estimate the test error by holding out a subset of the training observations from the model fitting process, and then applying the statistical learning method to those held out observations

---

## Approach #1: Validation set 

* Randomly divide the available set up samples into two parts: a **training set** and a **validation set**
--

* Fit the model on the **training set**, calculate the prediction error on the **validation set**

--

.question[
If we have a **quantitative predictor** what metric would we use to calculate this test error?
]

--

* Often we use Mean Squared Error (MSE)

---


## Approach #1: Validation set 

* Randomly divide the available set up samples into two parts: a **training set** and a **validation set**
* Fit the model on the **training set**, calculate the prediction error on the **validation set**

.question[
If we have a **qualitative predictor** (classification) what metric would we use to calculate this test error?
]

--

* Often we use misclassification rate

---

## Approach #1: Validation set



&lt;img src="05-cv_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;

--

`$$\Large\color{orange}{MSE_{\texttt{test-split}} = \textrm{Ave}_{i\in\texttt{test-split}}[y_i-\hat{f}(x_i)]^2}$$`

--
`$$\Large\color{orange}{Err_{\texttt{test-split}} = \textrm{Ave}_{i\in\texttt{test-split}}I[y_i\neq \mathcal{\hat{C}}(x_i)]}$$`


---

## Approach #1: Validation set

Auto example: 
* We have 392 observations
* Trying to predict `mpg` from `horsepower`
* We can split the data in half and use 196 to fit the model and 196 to test 

![](05-cv_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;

---

## Approach #1: Validation set


&lt;img src="05-cv_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;

.right[
`\(\color{orange}{MSE_{\texttt{test-split}}}\)`
]

--

&lt;img src="05-cv_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;


.right[
`\(\color{orange}{MSE_{\texttt{test-split}}}\)`
]

--

&lt;img src="05-cv_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;

.right[
`\(\color{orange}{MSE_{\texttt{test-split}}}\)`
]

--

&lt;img src="05-cv_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;

.right[
`\(\color{orange}{MSE_{\texttt{test-split}}}\)`
]


---

## Approach #1: Validation set

Auto example: 
* We have 392 observations
* Trying to predict `mpg` from `horsepower`
* We can split the data in half and use 196 to fit the model and 196 to test - **what if we did this many times?**

![](05-cv_files/figure-html/unnamed-chunk-10-1.png)&lt;!-- --&gt;

---

## Approach #1: Validation set (Drawbacks)

* the validation estimate of the test error can be highly variable, depending on which observations are included in the training set and which observations are included in the validation set
--

* In the validation approach, only a subset of the observations (those that are included in the training set rather than in the validation set) are used to fit the model

--
* Therefore, the validation set error may tend to **overestimate** the test error for the model fit on the entire data set

---

## Approach #2: K-fold cross validation

ðŸ’¡ The idea is to do the following:

*  Randomly divide the data into `\(K\)` equal-sized parts
--

*  Leave out part `\(k\)`, fit the model to the other `\(K - 1\)` parts (combined)
--

*  Obtain predictions for the left-out `\(k\)`th part
--

*  Do this for each part `\(k = 1, 2,\dots K\)`, and then combine the result

---

## K-fold cross validation

&lt;img src="05-cv_files/figure-html/unnamed-chunk-11-1.png" style="display: block; margin: auto;" /&gt;

.right[
`\(\color{orange}{MSE_{\texttt{test-split-1}}}\)`
]

--

&lt;img src="05-cv_files/figure-html/unnamed-chunk-12-1.png" style="display: block; margin: auto;" /&gt;
.right[
`\(\color{orange}{MSE_{\texttt{test-split-2}}}\)`
]

--

&lt;img src="05-cv_files/figure-html/unnamed-chunk-13-1.png" style="display: block; margin: auto;" /&gt;
.right[
`\(\color{orange}{MSE_{\texttt{test-split-3}}}\)`
]

--

&lt;img src="05-cv_files/figure-html/unnamed-chunk-14-1.png" style="display: block; margin: auto;" /&gt;
.right[
`\(\color{orange}{MSE_{\texttt{test-split-4}}}\)`
]

--

.right[
**Take the mean of the `\(k\)` MSE values**
]

---

## Estimating prediction error (quantitative outcome)

* Split the data into K parts, where `\(C_1, C_2, \dots, C_k\)` indicate the indices of observations in part `\(k\)`

`$$CV_{(K)} = \sum_{k=1}^K\frac{n_k}{n}MSE_k$$`

--

* `\(MSE_k = \sum_{i \in C_k} (y_i - \hat{y}_i)^2/n_k\)`
--

* `\(n_k\)` is the number of observations in group `\(k\)`
* `\(\hat{y}_i\)` is the fit for observation `\(i\)` obtained from the data with the part `\(k\)` removed
--

* If we set `\(K = n\)`, we'd have `\(n-fold\)` cross validation which is the same as **leave-one-out cross validation** (LOOCV)

---

## Leave-one-out cross validation

&lt;img src="05-cv_files/figure-html/unnamed-chunk-15-1.png" style="display: block; margin: auto;" /&gt;
--
&lt;img src="05-cv_files/figure-html/unnamed-chunk-16-1.png" style="display: block; margin: auto;" /&gt;
--
&lt;img src="05-cv_files/figure-html/unnamed-chunk-17-1.png" style="display: block; margin: auto;" /&gt;
--
&lt;img src="05-cv_files/figure-html/unnamed-chunk-18-1.png" style="display: block; margin: auto;" /&gt;
--
&lt;img src="05-cv_files/figure-html/unnamed-chunk-19-1.png" style="display: block; margin: auto;" /&gt;
--
&lt;img src="05-cv_files/figure-html/unnamed-chunk-20-1.png" style="display: block; margin: auto;" /&gt;
--
&lt;img src="05-cv_files/figure-html/unnamed-chunk-21-1.png" style="display: block; margin: auto;" /&gt;

--

`$$\vdots$$`

&lt;img src="05-cv_files/figure-html/unnamed-chunk-22-1.png" style="display: block; margin: auto;" /&gt;

&lt;img src="05-cv_files/figure-html/unnamed-chunk-23-1.png" style="display: block; margin: auto;" /&gt;

---

&lt;!-- ## Special Case! --&gt;

&lt;!-- * With _linear_ regression, you can actually calculate the LOOCV error without having to iterate! --&gt;

&lt;!-- `$$CV_{(n)} = \frac{1}{n}\sum_{i=1}^n\left(\frac{y_i-\hat{y}_i}{1-h_i}\right)^2$$` --&gt;

&lt;!-- -- --&gt;

&lt;!-- * `\(\hat{y}_i\)` is the `\(i\)`th fitted value from the linear model  --&gt;
&lt;!-- -- --&gt;

&lt;!-- * `\(h_i\)` is the diagonal of the "hat" matrix (remember that! ðŸŽ©) --&gt;

&lt;!-- --- --&gt;

## Picking `\(K\)`

* `\(K\)` can vary from 2 (splitting the data in half each time) to `\(n\)` (LOOCV)
--

* LOOCV is sometimes useful but usually the estimates from each fold are very correlated, so their average can have a **high variance**
--

* A better choice tends to be `\(K=5\)` or `\(K=10\)`

---

## Bias variance trade-off

* Since each training set is only `\((K - 1)/K\)` as big as the original training set, the estimates of prediction error will typically be **biased** upward

--
* This bias is minimized when `\(K = n\)` (LOOCV), but this estimate has a **high variance**

--
* `\(K =5\)` or `\(K=10\)` provides a nice compromise for the bias-variance trade-off

---

## Approach #2: K-fold Cross Validation

Auto example: 
* We have 392 observations
* Trying to predict `mpg` from `horsepower`


![](05-cv_files/figure-html/fig1-1.png)&lt;!-- --&gt;



---

## Estimating prediction error (qualitative outcome)

* The premise is the same as cross valiation for quantitative outcomes
* Split the data into K parts, where `\(C_1, C_2, \dots, C_k\)` indicate the indices of observations in part `\(k\)`

`$$CV_K = \sum_{k=1}^K\frac{n_k}{n}Err_k$$`

--
* `\(Err_k = \sum_{i\in C_k}I(y_i\neq\hat{y}_i)/n_k\)` (missclassification rate)

--
* `\(n_k\)` is the number of observations in group `\(k\)`
* `\(\hat{y}_i\)` is the fit for observation `\(i\)` obtained from the data with the part `\(k\)` removed





    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
